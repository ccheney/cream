"""
Golden File Generation & Equivalence Testing

Generates golden input/output files for cross-language equivalence testing
between Python and TypeScript implementations.

See: docs/plans/20-research-to-production-pipeline.md - Phase 5

Usage:
    validator = EquivalenceValidator("factor-id")
    await validator.generate_golden_files(factor, params)
    # Then run TypeScript tests against the generated files

    # Or validate TypeScript output:
    result = await validator.validate_output(typescript_output)
"""

from __future__ import annotations

import json
import logging
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import TYPE_CHECKING, Any

import numpy as np
import polars as pl

if TYPE_CHECKING:
    from .strategies.base import ResearchFactor

logger = logging.getLogger(__name__)


@dataclass
class EquivalenceTestResult:
    """Result of equivalence testing."""

    passed: bool
    """Whether all comparisons passed within tolerance."""

    max_divergence: float
    """Maximum absolute divergence found."""

    mean_divergence: float
    """Mean absolute divergence."""

    failed_indices: list[int]
    """Indices where divergence exceeded tolerance."""

    total_comparisons: int
    """Total number of values compared."""

    tolerance: float
    """Tolerance used for comparison."""

    def summary(self) -> str:
        """Get a summary of the test result."""
        status = "PASSED" if self.passed else "FAILED"
        return (
            f"{status}: {self.total_comparisons} comparisons, "
            f"max divergence: {self.max_divergence:.6f}, "
            f"mean divergence: {self.mean_divergence:.6f}, "
            f"failed: {len(self.failed_indices)}"
        )


@dataclass
class GoldenMetadata:
    """Metadata for golden files."""

    factor_id: str
    """Factor identifier."""

    n_samples: int
    """Number of test samples."""

    tolerance: float
    """Tolerance for equivalence testing."""

    generated_at: str
    """ISO-8601 timestamp of generation."""

    python_version: str
    """Python version used for generation."""

    polars_version: str
    """Polars version used."""

    numpy_version: str
    """NumPy version used."""

    params: dict[str, Any]
    """Parameters used for generation."""

    edge_cases: list[str]
    """Description of edge cases included."""


class EquivalenceValidator:
    """
    Generate and validate golden files for cross-language equivalence testing.

    Golden files are generated by the Python implementation and used to
    validate TypeScript implementations produce identical outputs.

    Example:
        validator = EquivalenceValidator("rsi-mean-reversion")
        await validator.generate_golden_files(factor, {"period": 14})

        # Later, validate TypeScript output
        ts_output = [...]  # Output from TypeScript implementation
        result = await validator.validate_output(ts_output)
        assert result.passed
    """

    DEFAULT_TOLERANCE = 0.0001
    DEFAULT_N_SAMPLES = 1000

    def __init__(
        self,
        factor_id: str,
        golden_root: Path | str | None = None,
        tolerance: float | None = None,
    ) -> None:
        """
        Initialize the validator.

        Args:
            factor_id: Unique identifier for the factor
            golden_root: Root directory for golden files (default: packages/research/golden)
            tolerance: Maximum allowed divergence (default: 0.0001)
        """
        self.factor_id = factor_id
        self.tolerance = tolerance or self.DEFAULT_TOLERANCE

        if golden_root is None:
            # Default to packages/research/golden/<factor_id>
            self.golden_dir = Path(__file__).parent.parent / "golden" / factor_id
        else:
            self.golden_dir = Path(golden_root) / factor_id

    async def generate_golden_files(
        self,
        factor: ResearchFactor,
        params: dict[str, Any],
        n_samples: int | None = None,
    ) -> Path:
        """
        Generate golden input/output files for equivalence testing.

        Creates:
        - input_sample.parquet: Test input data
        - expected_output.parquet: Expected signal output
        - params.json: Parameters used
        - metadata.json: Generation metadata

        Args:
            factor: Research factor to generate files for
            params: Parameters to use
            n_samples: Number of test samples (default: 1000)

        Returns:
            Path to the golden directory
        """
        n = n_samples or self.DEFAULT_N_SAMPLES

        # Generate diverse test data
        test_data = self._generate_test_data(n)

        # Compute Python output
        factor.set_parameters(params)
        python_output = factor.compute_signal(test_data)

        # Create golden directory
        self.golden_dir.mkdir(parents=True, exist_ok=True)

        # Save input (without timestamp for cross-platform compatibility)
        # Save as both parquet (for Python) and JSON (for TypeScript)
        input_for_save = test_data.drop("timestamp")
        input_for_save.write_parquet(self.golden_dir / "input_sample.parquet")
        with open(self.golden_dir / "input_sample.json", "w") as f:
            json.dump(input_for_save.to_dicts(), f, indent=2)

        # Save output
        output_data = pl.DataFrame({"signal": python_output.to_list()})
        output_data.write_parquet(self.golden_dir / "expected_output.parquet")
        with open(self.golden_dir / "expected_output.json", "w") as f:
            json.dump(output_data.to_dicts(), f, indent=2)

        # Save params
        with open(self.golden_dir / "params.json", "w") as f:
            json.dump(params, f, indent=2)

        # Save metadata
        metadata = GoldenMetadata(
            factor_id=self.factor_id,
            n_samples=n,
            tolerance=self.tolerance,
            generated_at=datetime.now().isoformat(),
            python_version=sys.version,
            polars_version=pl.__version__,
            numpy_version=np.__version__,
            params=params,
            edge_cases=[
                "flat_period_100_110",
                "gap_up_10pct_at_200",
                "low_volume_300_310",
                "negative_returns_400_410",
            ],
        )

        with open(self.golden_dir / "metadata.json", "w") as f:
            json.dump(
                {
                    "factor_id": metadata.factor_id,
                    "n_samples": metadata.n_samples,
                    "tolerance": metadata.tolerance,
                    "generated_at": metadata.generated_at,
                    "python_version": metadata.python_version,
                    "polars_version": metadata.polars_version,
                    "numpy_version": metadata.numpy_version,
                    "params": metadata.params,
                    "edge_cases": metadata.edge_cases,
                },
                f,
                indent=2,
            )

        logger.info(f"Generated golden files in {self.golden_dir}")
        return self.golden_dir

    def _generate_test_data(self, n: int) -> pl.DataFrame:
        """
        Generate diverse test data covering edge cases.

        Edge cases included:
        - Flat periods (no price change)
        - Large gaps (10% move)
        - Very low volume
        - Negative returns series

        Args:
            n: Number of samples

        Returns:
            DataFrame with OHLCV columns
        """
        np.random.seed(42)  # Reproducible

        # Base OHLCV data with random walk
        close = 100 + np.cumsum(np.random.randn(n) * 0.5)
        high = close + np.abs(np.random.randn(n)) * 0.5
        low = close - np.abs(np.random.randn(n)) * 0.5
        open_ = low + np.random.rand(n) * (high - low)
        volume = np.random.uniform(1e6, 1e8, n)

        # Ensure OHLC relationships
        high = np.maximum.reduce([high, open_, close])
        low = np.minimum.reduce([low, open_, close])

        # Timestamps
        start_date = datetime(2024, 1, 1)
        timestamps = [start_date + timedelta(days=i) for i in range(n)]

        # Add edge cases
        # 1. Flat period (no change) at indices 100-110
        if n > 110:
            flat_price = 100.0
            close[100:110] = flat_price
            high[100:110] = flat_price
            low[100:110] = flat_price
            open_[100:110] = flat_price

        # 2. Large gap up (10%) at index 200
        if n > 201:
            close[200] = close[199] * 1.1
            high[200] = close[200] * 1.02
            low[200] = close[199] * 1.05
            open_[200] = close[199] * 1.05

        # 3. Very low volume at indices 300-310
        if n > 310:
            volume[300:310] = 1000

        # 4. Negative returns series at indices 400-410
        if n > 410:
            for i in range(400, 410):
                close[i] = close[i - 1] * 0.99
            high[400:410] = close[400:410] * 1.005
            low[400:410] = close[400:410] * 0.995
            open_[400:410] = close[400:410] * 1.002

        return pl.DataFrame(
            {
                "timestamp": timestamps,
                "open": open_.tolist(),
                "high": high.tolist(),
                "low": low.tolist(),
                "close": close.tolist(),
                "volume": volume.tolist(),
            }
        )

    async def validate_output(
        self,
        actual_output: list[float],
    ) -> EquivalenceTestResult:
        """
        Validate output against golden expected output.

        Args:
            actual_output: Output from TypeScript or other implementation

        Returns:
            EquivalenceTestResult with comparison details
        """
        # Load expected output
        expected_path = self.golden_dir / "expected_output.parquet"
        if not expected_path.exists():
            raise FileNotFoundError(f"Golden output not found: {expected_path}")

        expected_df = pl.read_parquet(expected_path)
        expected = expected_df["signal"].to_list()

        return self._compare_outputs(actual_output, expected)

    def _compare_outputs(
        self,
        actual: list[float],
        expected: list[float],
    ) -> EquivalenceTestResult:
        """
        Compare actual and expected outputs.

        Args:
            actual: Actual output values
            expected: Expected output values

        Returns:
            EquivalenceTestResult with comparison details
        """
        if len(actual) != len(expected):
            return EquivalenceTestResult(
                passed=False,
                max_divergence=float("inf"),
                mean_divergence=float("inf"),
                failed_indices=[],
                total_comparisons=0,
                tolerance=self.tolerance,
            )

        # Handle NaN values
        divergences = []
        failed_indices = []

        for i, (a, e) in enumerate(zip(actual, expected, strict=True)):
            # Both NaN = match
            if np.isnan(a) and np.isnan(e):
                divergences.append(0.0)
                continue

            # One NaN, one not = mismatch
            if np.isnan(a) or np.isnan(e):
                divergences.append(float("inf"))
                failed_indices.append(i)
                continue

            # Normal comparison
            div = abs(a - e)
            divergences.append(div)
            if div > self.tolerance:
                failed_indices.append(i)

        finite_divs = [d for d in divergences if np.isfinite(d)]

        return EquivalenceTestResult(
            passed=len(failed_indices) == 0,
            max_divergence=max(divergences) if divergences else 0.0,
            mean_divergence=float(np.mean(finite_divs)) if finite_divs else 0.0,
            failed_indices=failed_indices,
            total_comparisons=len(divergences),
            tolerance=self.tolerance,
        )

    def load_golden_input(self) -> pl.DataFrame:
        """Load the golden input data."""
        input_path = self.golden_dir / "input_sample.parquet"
        if not input_path.exists():
            raise FileNotFoundError(f"Golden input not found: {input_path}")
        return pl.read_parquet(input_path)

    def load_golden_params(self) -> dict[str, Any]:
        """Load the golden parameters."""
        params_path = self.golden_dir / "params.json"
        if not params_path.exists():
            raise FileNotFoundError(f"Golden params not found: {params_path}")
        with open(params_path) as f:
            return json.load(f)

    def load_golden_metadata(self) -> dict[str, Any]:
        """Load the golden metadata."""
        metadata_path = self.golden_dir / "metadata.json"
        if not metadata_path.exists():
            raise FileNotFoundError(f"Golden metadata not found: {metadata_path}")
        with open(metadata_path) as f:
            return json.load(f)


def compare_outputs(
    actual: list[float],
    expected: list[float],
    tolerance: float = 0.0001,
) -> EquivalenceTestResult:
    """
    Compare two output lists for equivalence.

    Convenience function for ad-hoc comparisons.

    Args:
        actual: Actual output values
        expected: Expected output values
        tolerance: Maximum allowed divergence

    Returns:
        EquivalenceTestResult with comparison details
    """
    validator = EquivalenceValidator("temp", tolerance=tolerance)
    return validator._compare_outputs(actual, expected)
